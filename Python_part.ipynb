{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T4_Iago_Akio_Saito",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f9a9f7ea0bfa42a29ed80424e6c5b947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7a4734aef8cd4a6ab22cfb20460d413f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_31b4d87b381e4e1c8f63d6522cd433b3",
              "IPY_MODEL_f21f0719a4dd4930b73bdb2bc57c42e6",
              "IPY_MODEL_28f375c204274f8984caa22c97dbfa29"
            ]
          }
        },
        "7a4734aef8cd4a6ab22cfb20460d413f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "31b4d87b381e4e1c8f63d6522cd433b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3bb218590cdf467f88a6aa4038e3802c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c556797c8301402aaa816fe721d85a40"
          }
        },
        "f21f0719a4dd4930b73bdb2bc57c42e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_78aeca4fbc74413fa92be6244eae705a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 22139423,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 22139423,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d9fc76978c964c7daee9e80e95af02a8"
          }
        },
        "28f375c204274f8984caa22c97dbfa29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bde7294b8480471985dade3347938b0e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 21.1M/21.1M [00:00&lt;00:00, 63.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e1ca02adb2324513adffb6dae6ef7010"
          }
        },
        "3bb218590cdf467f88a6aa4038e3802c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c556797c8301402aaa816fe721d85a40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "78aeca4fbc74413fa92be6244eae705a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d9fc76978c964c7daee9e80e95af02a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bde7294b8480471985dade3347938b0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e1ca02adb2324513adffb6dae6ef7010": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vUDwPFUH6rG"
      },
      "source": [
        "#**INFORMAÇÕES DO ALUNO:**\n",
        "\n",
        "###**NOME: Iago Akio Saito**\n",
        "###**RGA: 2020.1906.060-1**\n",
        "###**CURSO: Engenharia de software**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKMLHJ2kthFb",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "316003bd-baad-4760-f083-0df4cc6dd5d2"
      },
      "source": [
        "!pip install -q kaggle\n",
        "\n",
        "from google.colab import files \n",
        "files.upload() # upload kaggle.json file to colab environment\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# download the dataset\n",
        "!kaggle datasets download -d iagoasaito/animais\n",
        "!unzip -q animais.zip -d dataset\n",
        "!rm animais.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7e58bf58-9b4a-4b39-8f72-5692fbd10ea9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7e58bf58-9b4a-4b39-8f72-5692fbd10ea9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Downloading animais.zip to /content\n",
            " 86% 41.0M/47.4M [00:00<00:00, 57.7MB/s]\n",
            "100% 47.4M/47.4M [00:00<00:00, 64.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmd4-thZmI9T"
      },
      "source": [
        "def pil_loader(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert('RGB')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwF1jUkHqbxm"
      },
      "source": [
        "## Analyzing the data\n",
        "\n",
        "Let's use MobileNetV3 and MNASNet to train our model. We're going to use the implementation from [torchvision](https://pytorch.org/vision/stable/models.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejG5cmjfsA3i"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Msu2JJXy6kHZ"
      },
      "source": [
        "dataset = {'filepaths':[], 'labels':[]}\n",
        "for curpath, parent, files in os.walk('dataset/train'):\n",
        "  if files == []: continue\n",
        "  samples = len(files)\n",
        "  dataset['labels'].extend([os.path.basename(curpath)] * samples)\n",
        "  dataset['filepaths'].extend([os.path.join(curpath, f) for f in files])\n",
        "for curpath, parent, files in os.walk('dataset/valid'):\n",
        "  if files == []: continue\n",
        "  samples = len(files)\n",
        "  dataset['labels'].extend([os.path.basename(curpath)] * samples)\n",
        "  dataset['filepaths'].extend([os.path.join(curpath, f) for f in files])\n",
        "data = pd.DataFrame(dataset)\n",
        "\n",
        "test_dataset = {'filepaths':[], 'labels':[]}\n",
        "for curpath, parent, files in os.walk('dataset/test'):\n",
        "  if files == []: continue\n",
        "  samples = len(files)\n",
        "  test_dataset['labels'].extend([os.path.basename(curpath)] * samples)\n",
        "  test_dataset['filepaths'].extend([os.path.join(curpath, f) for f in files])\n",
        "test_data = pd.DataFrame(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "m5s-V6qxuIt0",
        "outputId": "93de3c64-254b-47c2-a394-c3de3e1f35cf"
      },
      "source": [
        "#removing wrong data\n",
        "birds = ('Cats', 'Dogs', 'MaleLions', 'Sparrows')\n",
        "to_remove = []\n",
        "for index, (filepath, label) in enumerate(data.values):\n",
        "  if label not in birds:\n",
        "    to_remove.append(index)\n",
        "\n",
        "data = data.drop(labels = to_remove, axis = 0)\n",
        "data.reset_index(inplace=True)\n",
        "data.tail(15)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>filepaths</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1303</th>\n",
              "      <td>1303</td>\n",
              "      <td>dataset/valid/Cats/27131803876_9af3edf5c7_n.jpg</td>\n",
              "      <td>Cats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1304</th>\n",
              "      <td>1304</td>\n",
              "      <td>dataset/valid/Cats/33522734484_655cb6c80c_n.jpg</td>\n",
              "      <td>Cats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1305</th>\n",
              "      <td>1305</td>\n",
              "      <td>dataset/valid/Cats/24119958367_69117845b1_n.jpg</td>\n",
              "      <td>Cats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1306</th>\n",
              "      <td>1306</td>\n",
              "      <td>dataset/valid/Cats/32862691805_6efa2d6284_n.jpg</td>\n",
              "      <td>Cats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1307</th>\n",
              "      <td>1307</td>\n",
              "      <td>dataset/valid/Cats/31389231292_e2444d0260_m.jpg</td>\n",
              "      <td>Cats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1308</th>\n",
              "      <td>1308</td>\n",
              "      <td>dataset/valid/Cats/30038389868_bc56a9057c_n.jpg</td>\n",
              "      <td>Cats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1309</th>\n",
              "      <td>1309</td>\n",
              "      <td>dataset/valid/Cats/29191475061_e2415892d0_n.jpg</td>\n",
              "      <td>Cats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1310</th>\n",
              "      <td>1310</td>\n",
              "      <td>dataset/valid/Cats/26853880879_3f3de6ba26_n.jpg</td>\n",
              "      <td>Cats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1311</th>\n",
              "      <td>1311</td>\n",
              "      <td>dataset/valid/Cats/25161907065_81766634dc_n.jpg</td>\n",
              "      <td>Cats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1312</th>\n",
              "      <td>1312</td>\n",
              "      <td>dataset/valid/Cats/38785964744_4eab8c0319_n.jpg</td>\n",
              "      <td>Cats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1313</th>\n",
              "      <td>1313</td>\n",
              "      <td>dataset/valid/Cats/31272891202_a0f36c4230_n.jpg</td>\n",
              "      <td>Cats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1314</th>\n",
              "      <td>1314</td>\n",
              "      <td>dataset/valid/Cats/38268795194_5861fd8b96_n.jpg</td>\n",
              "      <td>Cats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1315</th>\n",
              "      <td>1315</td>\n",
              "      <td>dataset/valid/Cats/33799804162_540af0a20a_n.jpg</td>\n",
              "      <td>Cats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1316</th>\n",
              "      <td>1316</td>\n",
              "      <td>dataset/valid/Cats/23554364098_2f558cf9f3_m.jpg</td>\n",
              "      <td>Cats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1317</th>\n",
              "      <td>1317</td>\n",
              "      <td>dataset/valid/Cats/28483340843_5d645d0a65_m.jpg</td>\n",
              "      <td>Cats</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      index                                        filepaths labels\n",
              "1303   1303  dataset/valid/Cats/27131803876_9af3edf5c7_n.jpg   Cats\n",
              "1304   1304  dataset/valid/Cats/33522734484_655cb6c80c_n.jpg   Cats\n",
              "1305   1305  dataset/valid/Cats/24119958367_69117845b1_n.jpg   Cats\n",
              "1306   1306  dataset/valid/Cats/32862691805_6efa2d6284_n.jpg   Cats\n",
              "1307   1307  dataset/valid/Cats/31389231292_e2444d0260_m.jpg   Cats\n",
              "1308   1308  dataset/valid/Cats/30038389868_bc56a9057c_n.jpg   Cats\n",
              "1309   1309  dataset/valid/Cats/29191475061_e2415892d0_n.jpg   Cats\n",
              "1310   1310  dataset/valid/Cats/26853880879_3f3de6ba26_n.jpg   Cats\n",
              "1311   1311  dataset/valid/Cats/25161907065_81766634dc_n.jpg   Cats\n",
              "1312   1312  dataset/valid/Cats/38785964744_4eab8c0319_n.jpg   Cats\n",
              "1313   1313  dataset/valid/Cats/31272891202_a0f36c4230_n.jpg   Cats\n",
              "1314   1314  dataset/valid/Cats/38268795194_5861fd8b96_n.jpg   Cats\n",
              "1315   1315  dataset/valid/Cats/33799804162_540af0a20a_n.jpg   Cats\n",
              "1316   1316  dataset/valid/Cats/23554364098_2f558cf9f3_m.jpg   Cats\n",
              "1317   1317  dataset/valid/Cats/28483340843_5d645d0a65_m.jpg   Cats"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g6UtnZZ-t3m"
      },
      "source": [
        "# converting label to number\n",
        "label_to_index = {}\n",
        "index_to_label = []\n",
        "for i, label in enumerate(set(data['labels'].values)):\n",
        "  label_to_index[label] = i\n",
        "  index_to_label.append(label)\n",
        "\n",
        "# generate labels file\n",
        "with open(\"labels.txt\", \"w\") as f:\n",
        "  for idx in index_to_label:\n",
        "    f.write(idx+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6lC0f7atn6S"
      },
      "source": [
        "# We must create a custom Dataset\n",
        "\n",
        "class AnimaisDataset(Dataset):\n",
        "    \"\"\"AnimaisDataset.\"\"\"\n",
        "    def __init__(self, filepaths, labels, transform=None):\n",
        "        self.filepaths = filepaths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = pil_loader(self.filepaths[idx])\n",
        "\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "\n",
        "        return x, label_to_index[self.labels[idx]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPqZN8btuILr"
      },
      "source": [
        "# define the image transformations \n",
        "\n",
        "train_tfms = transforms.Compose([\n",
        "                            transforms.CenterCrop((224, 224)),\n",
        "                            transforms.RandomHorizontalFlip(p=0.5),\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize([0.485, 0.456, 0.406], \n",
        "                                                 [0.229, 0.224, 0.225])\n",
        "             ])\n",
        "\n",
        "test_tfms = transforms.Compose([\n",
        "                            transforms.Resize((256, 256)),\n",
        "                            transforms.CenterCrop((224, 224)),\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize([0.485, 0.456, 0.406], \n",
        "                                                [0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISIEGK5p5OKd"
      },
      "source": [
        "## Training with Cross-validation\n",
        "\n",
        "Training with 3-fold cross-validation over the train and valid dataset.\n",
        "\n",
        "Our stopping criteria will be early stopping after 10 epochs without loss improve.\n",
        "\n",
        "We'll use two metrics: accuracy and f1-score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYzbcCQU5OKg"
      },
      "source": [
        "lr = 0.0004 \n",
        "n_splits = 3 \n",
        "STOP = 10 # early stopping after STOP epochs without val loss improvement\n",
        "\n",
        "filepaths = data['filepaths'].values\n",
        "labels = data['labels'].values\n",
        "\n",
        "num_classes = len(set(data['labels'].values))\n",
        "device = torch.device('cuda:0')\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle = True, random_state = 42)\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt6das9g5OKg"
      },
      "source": [
        "# evaluate model with accuracy\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "metric2func = {\n",
        "    'accuracy'  : metrics.accuracy_score,\n",
        "    'confusion' : metrics.confusion_matrix,\n",
        "    'f1_score'  : metrics.f1_score,\n",
        "    'precision' : metrics.precision_score,\n",
        "    'recall'    : metrics.recall_score\n",
        "}\n",
        "\n",
        "def evaluate(model, dataloader, metrics = ['accuracy']):\n",
        "    acc = 0\n",
        "    test_loss = 0\n",
        "    \n",
        "    real = []\n",
        "    pred = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for i, (x, y) in enumerate(dataloader):\n",
        "        real.extend(y)\n",
        "\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        outputs = model(x)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        \n",
        "        loss = loss_func(outputs, y)\n",
        "\n",
        "        test_loss += loss.item() * x.size(0)\n",
        "        pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    test_loss /= i+1\n",
        "    computed_metric = {}\n",
        "    for metric in metrics:\n",
        "      if metric == 'f1_score':\n",
        "        computed_metric[metric] = metric2func[metric](real, pred, average='micro')\n",
        "      elif metric == 'confusion':\n",
        "        computed_metric[metric] = metric2func[metric](real, pred)\n",
        "      elif metric == 'precision':\n",
        "        computed_metric[metric] = metric2func[metric](real, pred, average='micro')\n",
        "      elif metric == 'recall':\n",
        "        computed_metric[metric] = metric2func[metric](real, pred, average='micro')\n",
        "      else:\n",
        "        computed_metric[metric] = metric2func[metric](real, pred)\n",
        "\n",
        "\n",
        "    return test_loss, computed_metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwDr94Ye5Lpw"
      },
      "source": [
        "def train_model(model, train_dl, val_dl, epochs, loss_func, optimizer, show_info=5, thresh = 1e-3):\n",
        "  epoch = 1\n",
        "  last_best = -1\n",
        "  start = time.time()\n",
        "  while epoch < epochs:\n",
        "    train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    values = 0\n",
        "    for x, y in train_dl:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      output = model(x)\n",
        "      current_loss = loss_func(output, y)\n",
        "\n",
        "      current_loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss += current_loss.item() * x.size(0)\n",
        "      values += x.size(0)\n",
        "\n",
        "    train_loss /= values\n",
        "    elapsed = int(time.time()-start)\n",
        "\n",
        "    if val_dl is not None:\n",
        "      val_loss, metrics = evaluate(model, val_dl)\n",
        "      val_acc = metrics['accuracy']\n",
        "\n",
        "      if epoch%show_info == 0:\n",
        "          print(f\"\\tEpoch {epoch}/{epochs}: train_loss = {train_loss:.3f} \"\\\n",
        "            f\"val_loss {val_loss:.3f} val_acc {val_acc:.2f} \"\\\n",
        "            f\"time elapsed = {elapsed//60}m:{elapsed%60}s\")\n",
        "        \n",
        "      if last_best <= epoch-STOP:\n",
        "        print('Early stopping...')\n",
        "        break\n",
        "      if last_best == -1 or val_loss + thresh < last_best_loss:\n",
        "        last_best = epoch\n",
        "        last_best_loss = val_loss\n",
        "\n",
        "    else:\n",
        "      if epoch%show_info == 0:\n",
        "          print(f\"\\tEpoch {epoch}/{epochs}: train_loss = {train_loss:.3f} \"\\\n",
        "            f\"time elapsed = {elapsed//60}m:{elapsed%60}s\")  \n",
        "      if last_best <= epoch-STOP:\n",
        "        print('Early stopping...')\n",
        "        break\n",
        "      if last_best == -1 or train_loss + thresh < last_best_loss:\n",
        "        last_best = epoch\n",
        "        last_best_loss = train_loss  \n",
        "    \n",
        "    epoch += 1\n",
        "  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f9a9f7ea0bfa42a29ed80424e6c5b947",
            "7a4734aef8cd4a6ab22cfb20460d413f",
            "31b4d87b381e4e1c8f63d6522cd433b3",
            "f21f0719a4dd4930b73bdb2bc57c42e6",
            "28f375c204274f8984caa22c97dbfa29",
            "3bb218590cdf467f88a6aa4038e3802c",
            "c556797c8301402aaa816fe721d85a40",
            "78aeca4fbc74413fa92be6244eae705a",
            "d9fc76978c964c7daee9e80e95af02a8",
            "bde7294b8480471985dade3347938b0e",
            "e1ca02adb2324513adffb6dae6ef7010"
          ]
        },
        "id": "YvjLtIyswzC9",
        "outputId": "a6aa3b91-2280-4c14-a5ce-a1b9e24e32d1"
      },
      "source": [
        "# training with MobileNet V3 Large\n",
        "\n",
        "confusion_matrix = []\n",
        "precision = []\n",
        "recall = []\n",
        "f1_score = []\n",
        "\n",
        "for fold, (train_val, test) in enumerate(skf.split(filepaths, labels)):\n",
        "  print(f'Fold {fold+1}/{n_splits}\\n')\n",
        "\n",
        "  train, val = next(sss.split(np.zeros(len(train_val)), labels[train_val]))\n",
        "  train = train_val[train]\n",
        "  val = train_val[val]\n",
        "\n",
        "  X_train, X_val, X_test = filepaths[train], filepaths[val], filepaths[test]\n",
        "  y_train, y_val, y_test = labels[train]   , labels[val]   , labels[test]\n",
        "\n",
        "  # datasets\n",
        "  train_dataset = AnimaisDataset(X_train, y_train, train_tfms)\n",
        "  val_dataset   = AnimaisDataset(X_val, y_val, test_tfms)\n",
        "  test_dataset  = AnimaisDataset(X_test, y_test, test_tfms)\n",
        "\n",
        "  # dataloaders\n",
        "  train_dl = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "  val_dl = DataLoader(val_dataset, batch_size=128)\n",
        "  test_dl = DataLoader(test_dataset, batch_size=128)\n",
        "\n",
        "  # getting model\n",
        "  model = models.mobilenet_v3_large(pretrained = True)\n",
        "  model.classifier[3] = torch.nn.Linear(model.classifier[3].in_features, num_classes)\n",
        "  model = model.to(device)\n",
        "\n",
        "  # loss\n",
        "  loss_func = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  # optimizer\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "\n",
        "  # training\n",
        "  model = train_model(model, train_dl, val_dl, 500, loss_func, optimizer, 1)\n",
        "  \n",
        "  # metrics\n",
        "  _, metrics = evaluate(model, test_dl, metrics=['confusion', 'precision', 'recall', 'f1_score'])\n",
        "  test_confusion, test_precision, test_recall, test_f1 = metrics['confusion'], metrics['precision'], metrics['recall'], metrics['f1_score']\n",
        "  print(f'Model saved - fold {fold}: '\\\n",
        "        f'test_confusion = \\n{test_confusion}\\ntest_precision = {test_precision:.2f}\\ntest_recall = {test_recall:.2f}\\ntest_f1_score = {test_f1:.2f}\\n')\n",
        "\n",
        "  confusion_matrix.append(test_confusion)\n",
        "  precision.append(test_precision)\n",
        "  recall.append(test_recall)\n",
        "  f1_score.append(test_f1)\n",
        "\n",
        "  # saving model\n",
        "  torch.save(model.state_dict(), f'mobilenetv3_large_f{fold}.pt')\n",
        "\n",
        "precision = np.array(precision)\n",
        "recall = np.array(recall)\n",
        "f1_score = np.array(f1_score)\n",
        "n = 1\n",
        "print(f'MobileNet V3 Large - precision: {precision.mean():.3f} += {precision.std():.3f} \\nrecall: {recall.mean():.3f} += {recall.std():.3f} \\nf1_score: {f1_score.mean():.3f} += {f1_score.std():.3f}')\n",
        "\n",
        "print(f'Confusion Matrix, por época: ')\n",
        "for i in confusion_matrix:\n",
        "  print(f'{n}:\\n {i}\\n')\n",
        "  n+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/3\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9a9f7ea0bfa42a29ed80424e6c5b947",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/21.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch 1/500: train_loss = 0.746 val_loss 22.304 val_acc 0.89 time elapsed = 0m:8s\n",
            "\tEpoch 2/500: train_loss = 0.066 val_loss 8.649 val_acc 0.97 time elapsed = 0m:16s\n",
            "\tEpoch 3/500: train_loss = 0.010 val_loss 3.885 val_acc 0.99 time elapsed = 0m:25s\n",
            "\tEpoch 4/500: train_loss = 0.002 val_loss 4.770 val_acc 0.97 time elapsed = 0m:33s\n",
            "\tEpoch 5/500: train_loss = 0.001 val_loss 6.068 val_acc 0.97 time elapsed = 0m:42s\n",
            "\tEpoch 6/500: train_loss = 0.001 val_loss 5.551 val_acc 0.98 time elapsed = 0m:50s\n",
            "\tEpoch 7/500: train_loss = 0.001 val_loss 11.042 val_acc 0.94 time elapsed = 0m:59s\n",
            "\tEpoch 8/500: train_loss = 0.001 val_loss 5.830 val_acc 0.95 time elapsed = 1m:7s\n",
            "\tEpoch 9/500: train_loss = 0.001 val_loss 4.931 val_acc 0.95 time elapsed = 1m:16s\n",
            "\tEpoch 10/500: train_loss = 0.001 val_loss 4.011 val_acc 0.98 time elapsed = 1m:24s\n",
            "\tEpoch 11/500: train_loss = 0.001 val_loss 3.750 val_acc 0.99 time elapsed = 1m:33s\n",
            "\tEpoch 12/500: train_loss = 0.001 val_loss 4.514 val_acc 0.98 time elapsed = 1m:42s\n",
            "\tEpoch 13/500: train_loss = 0.002 val_loss 6.223 val_acc 0.98 time elapsed = 1m:50s\n",
            "\tEpoch 14/500: train_loss = 0.001 val_loss 4.589 val_acc 0.98 time elapsed = 1m:59s\n",
            "\tEpoch 15/500: train_loss = 0.002 val_loss 5.124 val_acc 0.97 time elapsed = 2m:7s\n",
            "\tEpoch 16/500: train_loss = 0.000 val_loss 3.855 val_acc 0.99 time elapsed = 2m:16s\n",
            "\tEpoch 17/500: train_loss = 0.001 val_loss 4.162 val_acc 0.99 time elapsed = 2m:24s\n",
            "\tEpoch 18/500: train_loss = 0.003 val_loss 5.206 val_acc 0.99 time elapsed = 2m:33s\n",
            "\tEpoch 19/500: train_loss = 0.000 val_loss 6.074 val_acc 0.98 time elapsed = 2m:41s\n",
            "\tEpoch 20/500: train_loss = 0.002 val_loss 8.898 val_acc 0.97 time elapsed = 2m:50s\n",
            "\tEpoch 21/500: train_loss = 0.000 val_loss 14.624 val_acc 0.97 time elapsed = 2m:59s\n",
            "Early stopping...\n",
            "Model saved - fold 0: test_confusion = \n",
            "[[151   0   1   1]\n",
            " [  1  89   1   2]\n",
            " [  1   0  93   0]\n",
            " [  8   0   0  92]]\n",
            "test_precision = 0.97\n",
            "test_recall = 0.97\n",
            "test_f1_score = 0.97\n",
            "\n",
            "Fold 2/3\n",
            "\n",
            "\tEpoch 1/500: train_loss = 0.754 val_loss 33.234 val_acc 0.86 time elapsed = 0m:8s\n",
            "\tEpoch 2/500: train_loss = 0.079 val_loss 33.337 val_acc 0.89 time elapsed = 0m:16s\n",
            "\tEpoch 3/500: train_loss = 0.020 val_loss 33.511 val_acc 0.91 time elapsed = 0m:25s\n",
            "\tEpoch 4/500: train_loss = 0.004 val_loss 33.052 val_acc 0.90 time elapsed = 0m:34s\n",
            "\tEpoch 5/500: train_loss = 0.001 val_loss 30.384 val_acc 0.91 time elapsed = 0m:43s\n",
            "\tEpoch 6/500: train_loss = 0.001 val_loss 27.525 val_acc 0.89 time elapsed = 0m:51s\n",
            "\tEpoch 7/500: train_loss = 0.000 val_loss 27.195 val_acc 0.89 time elapsed = 1m:0s\n",
            "\tEpoch 8/500: train_loss = 0.000 val_loss 23.097 val_acc 0.89 time elapsed = 1m:9s\n",
            "\tEpoch 9/500: train_loss = 0.000 val_loss 19.240 val_acc 0.90 time elapsed = 1m:18s\n",
            "\tEpoch 10/500: train_loss = 0.000 val_loss 15.322 val_acc 0.93 time elapsed = 1m:26s\n",
            "\tEpoch 11/500: train_loss = 0.000 val_loss 12.975 val_acc 0.95 time elapsed = 1m:35s\n",
            "\tEpoch 12/500: train_loss = 0.000 val_loss 13.041 val_acc 0.95 time elapsed = 1m:44s\n",
            "\tEpoch 13/500: train_loss = 0.000 val_loss 12.513 val_acc 0.97 time elapsed = 1m:53s\n",
            "\tEpoch 14/500: train_loss = 0.000 val_loss 12.131 val_acc 0.97 time elapsed = 2m:1s\n",
            "\tEpoch 15/500: train_loss = 0.000 val_loss 11.805 val_acc 0.97 time elapsed = 2m:10s\n",
            "\tEpoch 16/500: train_loss = 0.001 val_loss 11.405 val_acc 0.97 time elapsed = 2m:19s\n",
            "\tEpoch 17/500: train_loss = 0.001 val_loss 11.982 val_acc 0.98 time elapsed = 2m:27s\n",
            "\tEpoch 18/500: train_loss = 0.000 val_loss 13.954 val_acc 0.98 time elapsed = 2m:36s\n",
            "\tEpoch 19/500: train_loss = 0.001 val_loss 16.973 val_acc 0.98 time elapsed = 2m:45s\n",
            "\tEpoch 20/500: train_loss = 0.009 val_loss 22.970 val_acc 0.97 time elapsed = 2m:54s\n",
            "\tEpoch 21/500: train_loss = 0.001 val_loss 20.199 val_acc 0.94 time elapsed = 3m:2s\n",
            "\tEpoch 22/500: train_loss = 0.001 val_loss 28.645 val_acc 0.92 time elapsed = 3m:11s\n",
            "\tEpoch 23/500: train_loss = 0.008 val_loss 14.681 val_acc 0.95 time elapsed = 3m:20s\n",
            "\tEpoch 24/500: train_loss = 0.001 val_loss 10.270 val_acc 0.95 time elapsed = 3m:28s\n",
            "\tEpoch 25/500: train_loss = 0.001 val_loss 9.559 val_acc 0.95 time elapsed = 3m:37s\n",
            "\tEpoch 26/500: train_loss = 0.001 val_loss 9.437 val_acc 0.97 time elapsed = 3m:46s\n",
            "\tEpoch 27/500: train_loss = 0.016 val_loss 15.615 val_acc 0.93 time elapsed = 3m:55s\n",
            "\tEpoch 28/500: train_loss = 0.003 val_loss 11.267 val_acc 0.94 time elapsed = 4m:3s\n",
            "\tEpoch 29/500: train_loss = 0.004 val_loss 18.925 val_acc 0.94 time elapsed = 4m:12s\n",
            "\tEpoch 30/500: train_loss = 0.026 val_loss 27.453 val_acc 0.93 time elapsed = 4m:21s\n",
            "\tEpoch 31/500: train_loss = 0.022 val_loss 34.908 val_acc 0.88 time elapsed = 4m:29s\n",
            "\tEpoch 32/500: train_loss = 0.007 val_loss 19.348 val_acc 0.92 time elapsed = 4m:38s\n",
            "\tEpoch 33/500: train_loss = 0.005 val_loss 13.245 val_acc 0.94 time elapsed = 4m:47s\n",
            "\tEpoch 34/500: train_loss = 0.002 val_loss 12.647 val_acc 0.95 time elapsed = 4m:55s\n",
            "\tEpoch 35/500: train_loss = 0.001 val_loss 15.270 val_acc 0.95 time elapsed = 5m:4s\n",
            "\tEpoch 36/500: train_loss = 0.001 val_loss 15.971 val_acc 0.95 time elapsed = 5m:12s\n",
            "Early stopping...\n",
            "Model saved - fold 1: test_confusion = \n",
            "[[143   4   3   3]\n",
            " [  0  89   2   2]\n",
            " [  0   0  93   0]\n",
            " [  1   0   0  99]]\n",
            "test_precision = 0.97\n",
            "test_recall = 0.97\n",
            "test_f1_score = 0.97\n",
            "\n",
            "Fold 3/3\n",
            "\n",
            "\tEpoch 1/500: train_loss = 0.650 val_loss 18.597 val_acc 0.94 time elapsed = 0m:8s\n",
            "\tEpoch 2/500: train_loss = 0.052 val_loss 13.803 val_acc 0.95 time elapsed = 0m:16s\n",
            "\tEpoch 3/500: train_loss = 0.011 val_loss 14.628 val_acc 0.92 time elapsed = 0m:25s\n",
            "\tEpoch 4/500: train_loss = 0.001 val_loss 15.923 val_acc 0.91 time elapsed = 0m:34s\n",
            "\tEpoch 5/500: train_loss = 0.002 val_loss 13.710 val_acc 0.92 time elapsed = 0m:42s\n",
            "\tEpoch 6/500: train_loss = 0.004 val_loss 12.586 val_acc 0.95 time elapsed = 0m:51s\n",
            "\tEpoch 7/500: train_loss = 0.003 val_loss 24.102 val_acc 0.93 time elapsed = 1m:0s\n",
            "\tEpoch 8/500: train_loss = 0.005 val_loss 16.907 val_acc 0.95 time elapsed = 1m:9s\n",
            "\tEpoch 9/500: train_loss = 0.012 val_loss 26.204 val_acc 0.92 time elapsed = 1m:17s\n",
            "\tEpoch 10/500: train_loss = 0.008 val_loss 25.073 val_acc 0.92 time elapsed = 1m:26s\n",
            "\tEpoch 11/500: train_loss = 0.003 val_loss 20.704 val_acc 0.93 time elapsed = 1m:35s\n",
            "\tEpoch 12/500: train_loss = 0.001 val_loss 18.702 val_acc 0.93 time elapsed = 1m:44s\n",
            "\tEpoch 13/500: train_loss = 0.001 val_loss 17.262 val_acc 0.93 time elapsed = 1m:52s\n",
            "\tEpoch 14/500: train_loss = 0.001 val_loss 16.783 val_acc 0.93 time elapsed = 2m:1s\n",
            "\tEpoch 15/500: train_loss = 0.000 val_loss 15.575 val_acc 0.93 time elapsed = 2m:10s\n",
            "\tEpoch 16/500: train_loss = 0.003 val_loss 13.888 val_acc 0.94 time elapsed = 2m:19s\n",
            "Early stopping...\n",
            "Model saved - fold 2: test_confusion = \n",
            "[[152   0   1   0]\n",
            " [  7  85   1   0]\n",
            " [  1   0  92   0]\n",
            " [  4   2   1  93]]\n",
            "test_precision = 0.96\n",
            "test_recall = 0.96\n",
            "test_f1_score = 0.96\n",
            "\n",
            "MobileNet V3 Large - precision: 0.964 += 0.002 \n",
            "recall: 0.964 += 0.002 \n",
            "f1_score: 0.964 += 0.002\n",
            "Confusion Matrix, por época: \n",
            "1:\n",
            " [[151   0   1   1]\n",
            " [  1  89   1   2]\n",
            " [  1   0  93   0]\n",
            " [  8   0   0  92]]\n",
            "\n",
            "2:\n",
            " [[143   4   3   3]\n",
            " [  0  89   2   2]\n",
            " [  0   0  93   0]\n",
            " [  1   0   0  99]]\n",
            "\n",
            "3:\n",
            " [[152   0   1   0]\n",
            " [  7  85   1   0]\n",
            " [  1   0  92   0]\n",
            " [  4   2   1  93]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_cvJ1CXUVfK",
        "outputId": "16fd0690-abab-4a1b-8d52-1c229bb1ccc7"
      },
      "source": [
        "n=1\n",
        "print(f'Confusion Matrix, por época: ')\n",
        "for i in confusion_matrix:\n",
        "  print(f'{n}:\\n {i}\\n')\n",
        "  n+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix, por época: \n",
            "1:\n",
            " [[151   0   1   1]\n",
            " [  1  89   1   2]\n",
            " [  1   0  93   0]\n",
            " [  8   0   0  92]]\n",
            "\n",
            "2:\n",
            " [[143   4   3   3]\n",
            " [  0  89   2   2]\n",
            " [  0   0  93   0]\n",
            " [  1   0   0  99]]\n",
            "\n",
            "3:\n",
            " [[152   0   1   0]\n",
            " [  7  85   1   0]\n",
            " [  1   0  92   0]\n",
            " [  4   2   1  93]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiAyQBR68pf9"
      },
      "source": [
        "## Deploying\n",
        "\n",
        "After analyzing metrics from last section, we've to train a final model to be used on our Android app."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZj44-8N8s4h",
        "outputId": "93ecc44f-7e09-4f64-8379-1ce2fc6d3896"
      },
      "source": [
        "## Training final model with MobileNet V3 Large\n",
        "\n",
        "# datasets\n",
        "train_dataset = AnimaisDataset(filepaths, labels, train_tfms)\n",
        "\n",
        "# dataloaders\n",
        "train_dl = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "# getting model\n",
        "model = models.mobilenet_v3_large(pretrained = True)\n",
        "model.classifier[3] = torch.nn.Linear(model.classifier[3].in_features, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "# loss\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "\n",
        "# training\n",
        "model = train_model(model, train_dl, None, 20, loss_func, optimizer, 1)\n",
        "model.to(torch.device('cpu'))\n",
        "\n",
        "# saving model\n",
        "torch.save(model.state_dict(), f'mobilenetv3_large_deploy.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch 1/20: train_loss = 0.495 time elapsed = 0m:13s\n",
            "\tEpoch 2/20: train_loss = 0.021 time elapsed = 0m:27s\n",
            "\tEpoch 3/20: train_loss = 0.002 time elapsed = 0m:41s\n",
            "\tEpoch 4/20: train_loss = 0.001 time elapsed = 0m:55s\n",
            "\tEpoch 5/20: train_loss = 0.001 time elapsed = 1m:8s\n",
            "\tEpoch 6/20: train_loss = 0.001 time elapsed = 1m:22s\n",
            "\tEpoch 7/20: train_loss = 0.002 time elapsed = 1m:36s\n",
            "\tEpoch 8/20: train_loss = 0.001 time elapsed = 1m:50s\n",
            "\tEpoch 9/20: train_loss = 0.003 time elapsed = 2m:3s\n",
            "\tEpoch 10/20: train_loss = 0.008 time elapsed = 2m:17s\n",
            "\tEpoch 11/20: train_loss = 0.027 time elapsed = 2m:31s\n",
            "\tEpoch 12/20: train_loss = 0.009 time elapsed = 2m:45s\n",
            "\tEpoch 13/20: train_loss = 0.007 time elapsed = 2m:58s\n",
            "\tEpoch 14/20: train_loss = 0.006 time elapsed = 3m:12s\n",
            "\tEpoch 15/20: train_loss = 0.001 time elapsed = 3m:26s\n",
            "Early stopping...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqN1Bv3ZSUdN"
      },
      "source": [
        "# Converting to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRog6sOcS5Rb",
        "outputId": "ff4b9afa-e3da-4af5-99a5-d9ed6e0482f7"
      },
      "source": [
        "!pip install onnx onnxruntime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.10.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (12.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.7 MB 5.0 MB/s \n",
            "\u001b[?25hCollecting onnxruntime\n",
            "  Downloading onnxruntime-1.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.8 MB 35.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.10.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from onnx) (1.15.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnx) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx) (1.19.5)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (2.0)\n",
            "Installing collected packages: onnxruntime, onnx\n",
            "Successfully installed onnx-1.10.2 onnxruntime-1.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHDwke4oS1IE"
      },
      "source": [
        "import onnx\n",
        "import onnxruntime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TX2-3J7SXXa"
      },
      "source": [
        "model_pytorch = models.mobilenet_v3_large(pretrained = True)\n",
        "model_pytorch.classifier[3] = torch.nn.Linear(model_pytorch.classifier[3].in_features, num_classes)\n",
        "model_pytorch.load_state_dict(torch.load('mobilenetv3_large_deploy.pt', map_location='cpu'))\n",
        "model_pytorch.eval()\n",
        "\n",
        "dummy_input = torch.randn(1, 3, 224, 224, requires_grad=True)\n",
        "\n",
        "torch.onnx.export(model_pytorch,                                 # model being run\n",
        "                  dummy_input,                                   # model input (or a tuple for multiple inputs)\n",
        "                  \"mobilenetv3_large_deploy.onnx\",               # where to save the model (can be a file or file-like object)\n",
        "                  export_params=True,                            # store the trained parameter weights inside the model file\n",
        "                  opset_version=12,                              # the ONNX version to export the model to\n",
        "                  do_constant_folding=True,                      # whether to execute constant folding for optimization\n",
        "                  input_names = ['input'],                       # the model's input names\n",
        "                  output_names = ['output'],                     # the model's output names\n",
        "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
        "                                'output' : {0 : 'batch_size'}})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Mb5WEmLTac9"
      },
      "source": [
        "We must build the ORT from the ONNX model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZuJwm9qTdX6",
        "outputId": "e7b85a07-3901-4c25-8c0f-7584273cff47"
      },
      "source": [
        "!python -m onnxruntime.tools.convert_onnx_models_to_ort . --optimization_level basic"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting optimized ONNX model /content/mobilenetv3_large_deploy.onnx to ORT format model /content/mobilenetv3_large_deploy.basic.ort\n",
            "Converted 1 models. 0 failures.\n",
            "2021-11-30 13:13:59,090 ort_format_model.utils [INFO] - Processed /content/mobilenetv3_large_deploy.basic.ort\n",
            "2021-11-30 13:13:59,096 ort_format_model.utils [INFO] - Created config in /content/required_operators.config\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KA3ecgnzTtF-",
        "outputId": "6c03fd3e-1fbf-4e57-98f4-2a7ddce379ee"
      },
      "source": [
        "%ls "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdataset\u001b[0m/                            mobilenetv3_large_f0.pt\n",
            "kaggle.json                         mobilenetv3_large_f1.pt\n",
            "labels.txt                          mobilenetv3_large_f2.pt\n",
            "mobilenetv3_large_deploy.basic.ort  required_operators.config\n",
            "mobilenetv3_large_deploy.onnx       \u001b[01;34msample_data\u001b[0m/\n",
            "mobilenetv3_large_deploy.pt\n"
          ]
        }
      ]
    }
  ]
}